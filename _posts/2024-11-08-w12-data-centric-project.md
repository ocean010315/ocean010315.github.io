---
title: Week 12 - Data Centric 주제 분류 프로젝트 리포트 & 개인 회고
date: 2024-11-08 00:00:00 +09:00
categories: [Naver Boostcamp AI Tech 7기, 주간 학습 기록]
tags: [NLP, Data Centric]
use_math: true
---

## Data-Centric 주제 분류 프로젝트
세 번째 프로젝트가 끝났다. 첫 번째는 STS, 두 번째는 RAG, 세 번째는 Data-Centric이다. 이번 프로젝트는 2주 동안 짧게 진행됐고, 개인 회고를 제출하지 않는 프로젝트였기 때문에 블로그에 개인적으로 기록하고자 한다.  
코드는 [여기](https://github.com/boostcampaitech7/level2-nlp-datacentric-nlp-06)에서 확인  

### 프로젝트 개요
뉴스 기사 제목으로 **주제 분류**를 하는 프로젝트이다. 우리는 일상생활에서 미디어를 통해 많은 양의 뉴스와 정보를 접하는데, 방대한 정보 속에서 필요한 내용을 빠르게 찾을 수 있도록 할 수 있다. 또한 특정 주제의 데이터를 분류하고 분석해서 신뢰할 수 있는 정보를 골라내는 등 가짜 뉴스를 탐지할 때도 주제 분류 프로젝트는 중요한 역할을 수행한다.  
이번 프로젝트는 이러한 주제 분류를 **데이터의 관점**에서 수행한다. 모델의 종류를 바꾸고, 미세 조정을 하고, 하이퍼파라미터 튜닝을 하는 등의 모델 관점을 배제하고 '좋은 데이터란 무엇인가'를 고민하며 데이터로 하여금 모델의 성능을 향상한다.  

제공된 데이터에는 사람이 무작위로 노이즈를 삽입한 형태이다. 기존 텍스트의 20-80%까지 그 비율은 무작위로 설정되었으며, 글자 삭제, ASCII 코드에 해당하는 기호 삽입, 대체 등의 노이즈가 있다. 라벨 또한 무작위로 오답 라벨을 지정한 형태의 노이즈가 포함되었으며, 텍스트 노이즈와 라벨 노이즈는 서로 섞이지 않는다.  
테스트 데이터에는 노이즈가 없으며, 학습 데이터를 통한 전처리, 증강만으로 모델의 성능을 올려야 한다. **결과적으로 총 2800개의 학습 데이터 중 1600개에는 텍스트 오염, 1000개에는 라벨 오염이 있으며, 테스트 데이터셋은 총 30,000개이다.**

## 시도한 방법들과 성과
주로 LLaMA 모델을 사용한 프롬프트 엔지니어링을 시도했는데, 사용한 모델은 비교적 최신 모델인 LLaMA 3.2를 한국어로 fine-tuning한 `Bllossom/llama-3.2-Korean-Bllossom-3B`를 사용했다. 크기가 작긴 하지만, 제한된 환경에서 적절히 성과를 보여 선택했다.

### 데이터 분석
데이터를 직접 눈으로 검수하고 전처리하는 방법이 금지되어 규칙 기반으로 오염도를 측정했다. 노이즈는 ASCII 코드라는 것을 알고 있기 때문에 한자는 음차어로 대체하고 특수문자, 알파벳의 비율을 계산했다. 개별 데이터의 오염도가 20% 미만일 경우 거의 온전한 형태였으며, 40%까지는 알아볼 수 있는 것과 없는 것이 섞여 있었고, 그 이상부터는 알아볼 수 없는 형태였다.  

### 라벨 키워드 추출 - LLaMA 모델 프롬프팅
라벨을 정수 정보로만 알 수 있고, 매핑되는 대상 정보를 사용할 수 없었기 때문에 LLaMA 모델을 사용한 프롬프팅으로 라벨의 키워드를 추출했다. 추출한 키워드는 아래와 같다.  
```json
{
    0: "문화",
    1: "스포츠",
    2: "정치",
    3: "사회",
    4: "경제",
    5: "기술",
    6: "국제"
}
```

### 텍스트 전처리 - LLaMA 모델 프롬프팅
텍스트 노이즈를 제거하기 위해 라벨별로 일부 예시를 추출하여 few-shot 예시로 사용했다. 복구할 수 없으면 베이스라인 모델에게 키워드라도 입력할 수 있도록 앞서 추출한 라벨 키워드를 붙여 `{키워드} {오염된 텍스트}` 형태로 출력하도록 했다.

### 라벨 전처리 - LLaMA 모델 프롬프팅
잘못된 라벨을 바로 고치기 위해 오염도가 20% 미만인 데이터를 대상으로 다시 프롬프팅을 수행했다. 이 때 앞서 추출한 키워드와 무작위로 선정한 예시를 few-shot으로 사용하여 데이터가 어느 분야인지 추론하게 하였다. 여기까지 1차 작업을 마치니 baseline의 **f1-score가 0.5980에서 `0.6821`까지 상승**하였다.  

### 데이터 증강, 라벨 검증
1차 작업을 마친 뒤 validation dataset을 확인한 결과 label=3(사회)의 f1-score가 유독 낮고, 개수 또한 부족함을 확인했다. 전체적으로 라벨의 비율은 균형적이라고 했기 때문에 일부 라벨링이 잘못된 것으로 판단했다. 다시 LLaMA 모델 프롬프팅으로 label=3인 데이터를 100개 추가 생성한 뒤, 베이스라인 모델을 학습하여 logit값을 추출하여 CleanLab을 적용했다. 결과적으로 **f1-score이 `0.7664`까지 상승**했다.  

## 시도했으나 아쉬운 방법들
### 텍스트 삭제
LLaMA 모델로 프롬프팅해서 텍스트 전처리 후 생각보다 복구를 못하는 형태였다. 따라서 키워드를 매핑하기 전에 오염도가 심한 데이터를 약 1300개 가량 삭제했다. 하지만 classification을 위해서는 모델에게 적절한 키워드를 입력하는 형태로 변경하고자 앞서 추출한 키워드를 매핑하여 다시 학습 데이터셋에 포함하였다. 결과적으로 증강하기 이전의 f1-score가 0.7380이 되었다. 하지만 증강 이후에는 해당 데이터 포함 여부가 최종 성적에 큰 영향을 주지는 못했다.

### 데이터 증강
성적에 영향을 준 증강은 validation f1-score 확인 후 모델이 잘 못하는 부분에 대한 증강이었지만 이 외에도 데이터의 양을 늘리기 위해 EDA(Easy Data Augmentation) 중 SR(Synonym Replacement)을 수행했다. 유사한 단어로 교체할 때 베이스라인이 이해할 수 있는 단어여야 하기 때문에 베이스라인 모델인 `klue/bert-base`의 vocab을 사용했으며, 텍스트 임베딩은 한국어 SBERT인 `snunlp/KR-SBERT-V40K-klueNLI-augSTS`로 수행했다. 기존 데이터셋 개수의 두 배 가량 증강했지만 성능 변화는 거의 없었고 최종적으로는 배제하였다.

## 개인 회고
### 잘한점
프로젝트 주제가 Data-Centric인 만큼 데이터 전처리, 증강을 시도해볼 수 있어서 좋았다. 특히 작은 크기의 LLaMA를 프롬프팅 하는 과정에서 프롬프트 작성법에 대한 기준이 어느정도 생긴 것 같다. 프롬프트에 대한 틀을 잘 정리해두고 계속 발전해나가면 좋겠다고 생각했다.  
또한 프로젝트 마감 후 github 코드 정리, README 작성을 담당했는데, 프로젝트 구조를 체계적으로 정리할 수 있어서 좋았다. 별도로 README에 대한 양식도 정리해서 보관해야겠다.

### 아쉬운점
프로젝트 종료 후 다른 팀들의 얘기를 들어보니 데이터를 최소 10,000개 이상으로 증강했다고 한다. SR 기법으로 증강했을 때 성능의 큰 변화가 없었기에 유사한 의미의 텍스트는 모델의 과적합을 야기할 수 있다고 판단했는데, 그 반대의 결과가 나와서 의아했다. 분석은 못 해봤지만 SR은 단순히 같은 구조와 같은 의미의 데이터를 사용한 것이고, 다른 팀이 증강한 데이터들은 합성 데이터와 같이 보다 다양한 형태일 수 있겠다고 생각했다. 추가로 이번 프로젝트에서는 모델에 관련된 작업을 수행할 수 없기 때문에 epoch이 2회로 고정되어 있었는데, 기존과 거의 유사한 형태의 데이터라도 그 양을 훨씬 더 늘리고 서로 다른 batch로 지정하면 epoch 수가 적어도 여러 번 학습하는 효과가 있는지 궁금하다.  
또한 LLaMA 프롬프팅을 많이 시도해본 것은 좋았으나 보다 다양한 모델 탐색과 방법론 적용을 못한 것은 아쉬웠다. 특히 라벨 정제를 위해 CleanLab 외에도 Clustering 방식을 적용해보았어야 한다고 생각했다.  
대부분 시간에 쫓겨 적용하지 못해본 것들이라 프로젝트 수행 시 부족한 시간을 극복하기 위해서 접근 방법에 대한 직관과 다음 작업에 대한 판단력을 기르고 싶다.